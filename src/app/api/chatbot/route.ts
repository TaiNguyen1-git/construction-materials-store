import { NextRequest, NextResponse } from 'next/server'
import { prisma } from '@/lib/prisma'
import { createSuccessResponse, createErrorResponse } from '@/lib/api-types'
import { z } from 'zod'
import { AIService } from '@/lib/ai-service'
import { isAIEnabled } from '@/lib/ai-config'
import { RAGService } from '@/lib/rag-service'
import { buildEnhancedPrompt, buildUserMessage, type ChatContext } from '@/lib/ai-prompts-enhanced'
import { conversationMemory } from '@/lib/conversation-memory'
import { aiRecognition } from '@/lib/ai-material-recognition'
import { mlRecommendations } from '@/lib/ml-recommendations'
import { materialCalculator } from '@/lib/material-calculator-service'

const chatMessageSchema = z.object({
  message: z.string().optional(), // Optional if image is provided
  image: z.string().optional(), // Base64 image
  customerId: z.string().optional(),
  sessionId: z.string().min(1, 'Session ID is required'),
  context: z.object({
    currentPage: z.string().optional(),
    productId: z.string().optional(),
    categoryId: z.string().optional(),
  }).optional(),
}).refine(data => data.message || data.image, {
  message: 'Either message or image is required'
})

// Generate chatbot response using AI or fallback to mock
async function generateChatbotResponse(
  message: string, 
  context?: any,
  conversationHistory?: { role: string; content: string }[]
): Promise<{
  response: string;
  suggestions: string[];
  productRecommendations?: any[];
  confidence: number;
  materialCalculation?: any;
}> {
  // Use real AI with RAG if enabled, otherwise fallback to mock
  if (isAIEnabled()) {
    try {
      // ===== ENHANCED PROMPTS + RAG =====
      // Build enhanced context with conversation memory
      const chatContext: ChatContext = {
        customerContext: context?.customerId 
          ? await conversationMemory.getUserContext(context.customerId)
          : undefined,
        currentPage: context?.currentPage,
        sessionContext: {
          previousQueries: conversationHistory?.slice(-5).map(h => h.content) || [],
          language: 'vi'
        }
      }

      // Get relevant context from knowledge base
      const augmentedMessage = await RAGService.generateAugmentedPrompt(message, conversationHistory)
      
      // Build enhanced system prompt
      const enhancedSystemPrompt = buildEnhancedPrompt(message, chatContext)
      const userMessage = buildUserMessage(message, chatContext)
      
      // Get AI response with enhanced context
      const aiResponse = await AIService.generateChatbotResponse(
        enhancedSystemPrompt + '\n\n' + augmentedMessage, 
        context, 
        conversationHistory
      )
      
      // Get product recommendations from knowledge base
      const knowledgeProducts = await RAGService.getProductRecommendations(message, 3)
      
      // Format knowledge base products for response
      const knowledgeBasedRecommendations = knowledgeProducts.map(p => ({
        id: p.id,
        name: p.name,
        brand: p.brand,
        price: p.pricing.basePrice,
        unit: p.pricing.unit,
        description: p.description,
        inStock: true // Assume in stock
      }))
      
      // Merge with AI recommendations
      const allRecommendations = [
        ...knowledgeBasedRecommendations,
        ...(aiResponse.productRecommendations || [])
      ]
      
      // Deduplicate by name
      const uniqueRecommendations = allRecommendations.filter((item, index, self) =>
        index === self.findIndex(t => t.name === item.name)
      ).slice(0, 5)
      
      return {
        ...aiResponse,
        productRecommendations: uniqueRecommendations,
        confidence: Math.min(0.98, aiResponse.confidence + 0.1) // RAG increases confidence
      }
    } catch (error) {
      console.error('AI service error, falling back to mock:', error)
      // Fallback to mock response
    }
  }
  
  // Mock AI response function (in production, this would integrate with OpenAI or similar)
  // Simulate processing time
  await new Promise(resolve => setTimeout(resolve, 1000))
  
  const lowerMessage = message.toLowerCase()
  const lowerMessageVi = message.toLowerCase() // Preserve Vietnamese
  
  // ===== TRY RAG FIRST FOR PRODUCT QUERIES =====
  try {
    const relevantProducts = await RAGService.retrieveContext(message, 2)
    if (relevantProducts.length > 0) {
      const product = relevantProducts[0]
      const formattedResponse = RAGService.formatProductForChat(product)
      
      // Get cross-sell products
      const crossSell = await RAGService.getCrossSellProducts(product.id)
      
      return {
        response: formattedResponse,
        suggestions: [
          'Xem thÃªm chi tiáº¿t',
          'TÃ­nh toÃ¡n váº­t liá»‡u',
          crossSell.length > 0 ? `Xem ${crossSell[0].name}` : 'Sáº£n pháº©m khÃ¡c',
          'LiÃªn há»‡ tÆ° váº¥n'
        ],
        productRecommendations: [
          {
            id: product.id,
            name: product.name,
            brand: product.brand,
            price: product.pricing.basePrice,
            unit: product.pricing.unit
          },
          ...crossSell.slice(0, 2).map(p => ({
            id: p.id,
            name: p.name,
            brand: p.brand,
            price: p.pricing.basePrice,
            unit: p.pricing.unit
          }))
        ],
        confidence: 0.95
      }
    }
  } catch (ragError) {
    console.log('RAG search failed, falling back to mock:', ragError)
  }

  // Price inquiry (Vietnamese & English)
  if (lowerMessage.includes('price') || lowerMessage.includes('cost') || lowerMessage.includes('how much') ||
      lowerMessageVi.includes('giÃ¡') || lowerMessageVi.includes('bao nhiÃªu') || lowerMessageVi.includes('chi phÃ­')) {
    if (lowerMessage.includes('cement') || lowerMessageVi.includes('xi mÄƒng') || lowerMessageVi.includes('xi mang')) {
      return {
        response: "GiÃ¡ xi mÄƒng cá»§a chÃºng tÃ´i tÃ¹y loáº¡i vÃ  sá»‘ lÆ°á»£ng áº¡:\n\n" +
          "â€¢ **Xi mÄƒng PC30** (vá»¯a xÃ¢y): 105.000Ä‘/bao 50kg\n" +
          "â€¢ **Xi mÄƒng PC40** (bÃª tÃ´ng): 120.000Ä‘/bao 50kg\n" +
          "â€¢ **Xi mÄƒng PCB40** (bÃª tÃ´ng cao cáº¥p): 135.000Ä‘/bao 50kg\n\n" +
          "Äáº·t tá»« 100 bao trá»Ÿ lÃªn Ä‘Æ°á»£c **giáº£m 10%**. Báº¡n cáº§n bao nhiÃªu bao Ä‘á»ƒ mÃ¬nh bÃ¡o giÃ¡ chi tiáº¿t nhÃ©?",
        suggestions: ["BÃ¡o giÃ¡ sá»‘ lÆ°á»£ng lá»›n", "Xem cÃ¡c loáº¡i xi mÄƒng", "TÃ­nh tá»•ng chi phÃ­"],
        confidence: 0.95
      }
    } else if (lowerMessage.includes('steel') || lowerMessage.includes('rebar') || 
               lowerMessageVi.includes('thÃ©p') || lowerMessageVi.includes('sáº¯t')) {
      return {
        response: "GiÃ¡ thÃ©p phá»¥ thuá»™c vÃ o Ä‘Æ°á»ng kÃ­nh vÃ  chiá»u dÃ i áº¡:\n\n" +
          "â€¢ **ThÃ©p D6-D8**: 16.000Ä‘/kg\n" +
          "â€¢ **ThÃ©p D10-D12**: 17.500Ä‘/kg\n" +
          "â€¢ **ThÃ©p D16-D18**: 18.500Ä‘/kg\n" +
          "â€¢ **ThÃ©p D20-D25**: 19.000Ä‘/kg\n\n" +
          "Chiá»u dÃ i tiÃªu chuáº©n 6m hoáº·c 12m. Báº¡n cáº§n quy cÃ¡ch nÃ o Ä‘á»ƒ mÃ¬nh tÆ° váº¥n chi tiáº¿t?",
        suggestions: ["Xem catalog thÃ©p", "GiÃ¡ sá»‰", "Kiá»ƒm tra tá»“n kho"],
        confidence: 0.92
      }
    } else if (lowerMessageVi.includes('gáº¡ch')) {
      return {
        response: "CÃ³ nhiá»u loáº¡i gáº¡ch vá»›i giÃ¡ khÃ¡c nhau áº¡:\n\n" +
          "â€¢ **Gáº¡ch 4 lá»—** 8x8x18cm: 2.200Ä‘/viÃªn\n" +
          "â€¢ **Gáº¡ch Ä‘áº·c**: 3.500Ä‘/viÃªn\n" +
          "â€¢ **Gáº¡ch block 10cm**: 8.500Ä‘/viÃªn\n" +
          "â€¢ **Gáº¡ch á»‘p lÃ¡t** 60x60cm: 85.000Ä‘/mÂ²\n\n" +
          "Báº¡n cáº§n gáº¡ch loáº¡i nÃ o Ä‘á»ƒ mÃ¬nh tÆ° váº¥n cá»¥ thá»ƒ hÆ¡n nhÃ©?",
        suggestions: ["Gáº¡ch xÃ¢y tÆ°á»ng", "Gáº¡ch lÃ¡t ná»n", "Gáº¡ch block"],
        confidence: 0.93
      }
    } else {
      return {
        response: "TÃ´i sáºµn sÃ ng bÃ¡o giÃ¡ cho báº¡n! Báº¡n muá»‘n biáº¿t giÃ¡ cá»§a váº­t liá»‡u nÃ o áº¡? ChÃºng tÃ´i cÃ³:\n\n" +
          "ðŸ§± Xi mÄƒng, thÃ©p, cÃ¡t, Ä‘Ã¡, gáº¡ch\n" +
          "ðŸ  NgÃ³i, tÃ´n, vÃ¡n Ã©p\n" +
          "ðŸŽ¨ SÆ¡n, bá»™t trÃ©t, chá»‘ng tháº¥m\n" +
          "ðŸ”§ CÃ´ng cá»¥ vÃ  váº­t tÆ° khÃ¡c\n\n" +
          "Cho mÃ¬nh biáº¿t báº¡n cáº§n váº­t liá»‡u gÃ¬ nhÃ©!",
        suggestions: ["Xi mÄƒng", "ThÃ©p", "Gáº¡ch", "CÃ¡t & ÄÃ¡", "Danh má»¥c sáº£n pháº©m"],
        confidence: 0.85
      }
    }
  }
  
  // Stock/availability inquiry
  if (lowerMessage.includes('stock') || lowerMessage.includes('available') || lowerMessage.includes('in stock')) {
    return {
      response: "I can check our current stock levels for you. We update our inventory in real-time. Which specific products are you looking for? You can also browse our online catalog to see current availability.",
      suggestions: ["Check cement stock", "Check steel availability", "View all products"],
      confidence: 0.90
    }
  }
  
  // Store hours inquiry
  if (lowerMessage.includes('hours') || lowerMessage.includes('open') || lowerMessage.includes('close') || lowerMessage.includes('time')) {
    return {
      response: "Our store hours are Monday-Friday: 7:00 AM - 6:00 PM, Saturday: 8:00 AM - 4:00 PM, Sunday: Closed. We also offer 24/7 online ordering with next-day pickup available.",
      suggestions: ["Place online order", "Schedule pickup", "Contact us"],
      confidence: 0.98
    }
  }
  
  // Delivery inquiry
  if (lowerMessage.includes('delivery') || lowerMessage.includes('shipping') || lowerMessage.includes('deliver')) {
    return {
      response: "Yes, we offer delivery services! Free delivery for orders over $500 within 10 miles. For smaller orders or longer distances, delivery fees apply. Delivery is typically within 1-2 business days. Would you like to check delivery options for your location?",
      suggestions: ["Check delivery cost", "Schedule delivery", "View delivery areas"],
      confidence: 0.94
    }
  }
  
  // Product recommendations
  if (lowerMessage.includes('recommend') || lowerMessage.includes('best') || lowerMessage.includes('need')) {
    if (lowerMessage.includes('foundation') || lowerMessage.includes('concrete')) {
      return {
        response: "For foundation work, I recommend our premium concrete mix and steel rebar for reinforcement. You'll also need gravel for the base and waterproofing materials. Would you like me to create a foundation materials package for you?",
        suggestions: ["Foundation package", "Calculate quantities", "Get quote"],
        productRecommendations: [
          { name: "Premium Concrete Mix", price: 25.00, unit: "bag" },
          { name: "Steel Rebar 12mm", price: 8.50, unit: "piece" },
          { name: "Waterproof Membrane", price: 45.00, unit: "roll" }
        ],
        confidence: 0.88
      }
    } else {
      return {
        response: "I'd be happy to recommend the right materials for your project! Could you tell me more about what you're building or working on? For example, are you doing foundation work, roofing, walls, or something else?",
        suggestions: ["Foundation materials", "Roofing supplies", "Wall materials"],
        confidence: 0.82
      }
    }
  }
  
  // Greeting (Vietnamese & English)
  if (lowerMessage.includes('hello') || lowerMessage.includes('hi') || lowerMessage.includes('hey') ||
      lowerMessageVi.includes('xin chÃ o') || lowerMessageVi.includes('chÃ o') || lowerMessageVi.includes('hello')) {
    return {
      response: "Xin chÃ o! ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i SmartBuild AI. TÃ´i lÃ  trá»£ lÃ½ áº£o, sáºµn sÃ ng giÃºp báº¡n tÃ¬m váº­t liá»‡u phÃ¹ há»£p cho cÃ´ng trÃ¬nh. Báº¡n Ä‘ang cáº§n tÆ° váº¥n gÃ¬ áº¡?",
      suggestions: ["TÃ­nh toÃ¡n váº­t liá»‡u", "Xem sáº£n pháº©m", "Kiá»ƒm tra giÃ¡", "ThÃ´ng tin cá»­a hÃ ng"],
      confidence: 0.96
    }
  }
  
  // Material calculation request
  if (lowerMessageVi.includes('tÃ­nh') && (lowerMessageVi.includes('váº­t liá»‡u') || lowerMessageVi.includes('xi mÄƒng') || lowerMessageVi.includes('gáº¡ch'))) {
    return {
      response: "Tuyá»‡t vá»i! TÃ´i cÃ³ thá»ƒ giÃºp báº¡n tÃ­nh toÃ¡n chÃ­nh xÃ¡c váº­t liá»‡u cáº§n thiáº¿t. Äá»ƒ tÃ­nh toÃ¡n tá»‘t nháº¥t, cho tÃ´i biáº¿t:\n\n" +
        "ðŸ  **Loáº¡i cÃ´ng trÃ¬nh**: NhÃ  phá»‘ / Biá»‡t thá»± / NhÃ  xÆ°á»Ÿng / Chung cÆ°?\n" +
        "ðŸ“ **Diá»‡n tÃ­ch**: Bao nhiÃªu mÂ²?\n" +
        "ðŸ—ï¸ **Sá»‘ táº§ng**: Bao nhiÃªu táº§ng?\n" +
        "ðŸ§± **Loáº¡i tÆ°á»ng**: Gáº¡ch / BÃª tÃ´ng?\n" +
        "ðŸ  **Loáº¡i mÃ¡i**: NgÃ³i / TÃ´n / BÃª tÃ´ng?\n\n" +
        "Hoáº·c báº¡n cÃ³ thá»ƒ vÃ o má»¥c **TÃ­nh toÃ¡n váº­t liá»‡u** trÃªn website Ä‘á»ƒ nháº­p Ä‘áº§y Ä‘á»§ thÃ´ng tin nhÃ©!",
      suggestions: ["NhÃ  phá»‘ 100mÂ²", "Biá»‡t thá»± 200mÂ²", "NhÃ  xÆ°á»Ÿng 500mÂ²", "TÃ­nh toÃ¡n chi tiáº¿t"],
      confidence: 0.95
    }
  }
  
  // Order inquiry
  if (lowerMessage.includes('order') || lowerMessage.includes('buy') || lowerMessage.includes('purchase')) {
    return {
      response: "Great! You can place orders online through our website or visit our store. We accept cash, card, and bank transfers. For large orders, we also offer credit terms for registered customers. What would you like to order?",
      suggestions: ["Browse products", "Create account", "Contact sales"],
      confidence: 0.91
    }
  }
  
  // Default response
  return {
    response: "I'm here to help with information about our construction materials, pricing, availability, store hours, and delivery options. Could you please rephrase your question or let me know what specific information you're looking for?",
    suggestions: ["View products", "Check prices", "Store hours", "Delivery info"],
    confidence: 0.75
  }
}

// POST /api/chatbot - Process chatbot message (with image support)
export async function POST(request: NextRequest) {
  try {
    const body = await request.json()
    
    // Validate input
    const validation = chatMessageSchema.safeParse(body)
    if (!validation.success) {
      return NextResponse.json(
        createErrorResponse('Invalid input', 'VALIDATION_ERROR', validation.error.issues),
        { status: 400 }
      )
    }

    const { message, image, customerId, sessionId, context } = validation.data

    // Get conversation history for context-aware responses
    const conversationHistory = await prisma.customerInteraction.findMany({
      where: {
        sessionId,
        interactionType: 'CHATBOT',
        createdAt: {
          gte: new Date(Date.now() - 3600000) // Last 1 hour
        }
      },
      orderBy: { createdAt: 'asc' },
      take: 10, // Last 10 messages
      select: {
        query: true,
        response: true
      }
    })

    // Format conversation history for AI
    const formattedHistory: { role: string; content: string }[] = []
    conversationHistory.forEach(interaction => {
      formattedHistory.push({ role: 'user', content: interaction.query })
      formattedHistory.push({ role: 'assistant', content: interaction.response })
    })

    let botResponse: any
    let actualQuery = message || ''
    let recognitionResult: any = null

    // ===== IMAGE RECOGNITION FLOW =====
    if (image) {
      console.log('ðŸ“¸ Processing image with AI recognition...')
      
      try {
        // Recognize material from image
        recognitionResult = await aiRecognition.recognizeMaterial(image)
        
        console.log(`âœ… Recognized: ${recognitionResult.materialType} (${(recognitionResult.confidence * 100).toFixed(0)}%)`)
        
        // Build natural language response
        let responseText = `ðŸ“¸ **TÃ´i nháº­n diá»‡n Ä‘Æ°á»£c:** ${recognitionResult.materialType}\n\n`
        responseText += `ðŸŽ¯ **Äá»™ tin cáº­y:** ${(recognitionResult.confidence * 100).toFixed(0)}%\n\n`
        
        if (recognitionResult.matchedProducts.length > 0) {
          responseText += `âœ… **TÃ¬m tháº¥y ${recognitionResult.matchedProducts.length} sáº£n pháº©m phÃ¹ há»£p:**\n\n`
          
          // Get ML-enhanced recommendations for these products
          const productIds = recognitionResult.matchedProducts.map((p: any) => p.id)
          let enhancedProducts = recognitionResult.matchedProducts
          
          // Try to enhance with ML recommendations if customer ID available
          if (customerId && productIds.length > 0) {
            try {
              const mlScores = await mlRecommendations.getHybridRecommendations(
                productIds[0], // Use first product as reference
                customerId,
                'SIMILAR',
                5
              )
              enhancedProducts = await mlRecommendations.enrichRecommendations(mlScores)
              console.log('ðŸ¤– Enhanced with ML recommendations')
            } catch (mlError) {
              console.log('Using original recognition results (ML enhancement failed)')
            }
          }
          
          botResponse = {
            response: responseText,
            suggestions: recognitionResult.suggestions,
            productRecommendations: enhancedProducts,
            confidence: recognitionResult.confidence,
            recognitionData: recognitionResult.features
          }
        } else {
          responseText += 'âŒ KhÃ´ng tÃ¬m tháº¥y sáº£n pháº©m phÃ¹ há»£p trong kho.\n\n'
          responseText += recognitionResult.suggestions.join('\n')
          
          botResponse = {
            response: responseText,
            suggestions: ['Thá»­ chá»¥p áº£nh khÃ¡c', 'TÃ¬m kiáº¿m báº±ng text', 'Xem danh má»¥c sáº£n pháº©m'],
            productRecommendations: [],
            confidence: recognitionResult.confidence
          }
        }
        
        // If user also included a message, append to context
        if (message) {
          actualQuery = `[Gá»­i áº£nh ${recognitionResult.materialType}] ${message}`
        } else {
          actualQuery = `[Gá»­i áº£nh ${recognitionResult.materialType}]`
        }
      } catch (recognitionError) {
        console.error('AI recognition error:', recognitionError)
        botResponse = {
          response: 'ðŸ˜… Xin lá»—i, tÃ´i gáº·p khÃ³ khÄƒn khi nháº­n diá»‡n áº£nh nÃ y. Báº¡n cÃ³ thá»ƒ chá»¥p láº¡i rÃµ hÆ¡n hoáº·c nháº­p tÃªn váº­t liá»‡u khÃ´ng?',
          suggestions: ['Chá»¥p láº¡i áº£nh', 'TÃ¬m kiáº¿m báº±ng text', 'Xem danh má»¥c'],
          productRecommendations: [],
          confidence: 0.5
        }
        actualQuery = '[Gá»­i áº£nh - nháº­n diá»‡n tháº¥t báº¡i]'
      }
    } 
    // ===== TEXT-ONLY FLOW =====
    else {
      const lowerMessage = message?.toLowerCase() || ''
      
      // ===== MATERIAL CALCULATOR FLOW =====
      if (lowerMessage.includes('tÃ­nh') && (
          lowerMessage.includes('váº­t liá»‡u') ||
          lowerMessage.includes('mÂ²') || lowerMessage.includes('m2') ||
          lowerMessage.includes('táº§ng') || lowerMessage.includes('tang') ||
          lowerMessage.includes('nhÃ ') || lowerMessage.includes('nha')
      )) {
        console.log('ðŸ§® Material calculation request detected...')
        
        try {
          const calcInput = materialCalculator.parseQuery(message!)
          
          if (calcInput) {
            const calcResult = await materialCalculator.quickCalculate(calcInput)
            const formattedResponse = materialCalculator.formatForChat(calcResult)
            
            botResponse = {
              response: formattedResponse,
              suggestions: [
                'Äiá»u chá»‰nh tÃ­nh toÃ¡n',
                'Xem sáº£n pháº©m xi mÄƒng',
                'Xem sáº£n pháº©m gáº¡ch',
                'TÆ° váº¥n thÃªm'
              ],
              productRecommendations: [],
              confidence: 0.92,
              calculationData: calcResult
            }
          } else {
            botResponse = {
              response: `ðŸ§® TÃ´i cÃ³ thá»ƒ giÃºp báº¡n tÃ­nh toÃ¡n váº­t liá»‡u!\n\n` +
                       `Vui lÃ²ng cung cáº¥p thÃ´ng tin:\n` +
                       `â€¢ Diá»‡n tÃ­ch hoáº·c kÃ­ch thÆ°á»›c (VD: 100mÂ², 10x15m)\n` +
                       `â€¢ Sá»‘ táº§ng (VD: 2 táº§ng)\n` +
                       `â€¢ Loáº¡i cÃ´ng trÃ¬nh (VD: nhÃ  phá»‘, biá»‡t thá»±)\n\n` +
                       `**VÃ­ dá»¥:** "TÃ­nh váº­t liá»‡u nhÃ  phá»‘ 100mÂ² 2 táº§ng"`,
              suggestions: [
                'TÃ­nh nhÃ  phá»‘ 100mÂ²',
                'TÃ­nh biá»‡t thá»± 200mÂ² 2 táº§ng',
                'TÃ­nh nhÃ  xÆ°á»Ÿng 500mÂ²'
              ],
              productRecommendations: [],
              confidence: 0.85
            }
          }
        } catch (calcError: any) {
          console.error('Calculation error:', calcError)
          botResponse = {
            response: `âŒ Lá»—i tÃ­nh toÃ¡n: ${calcError.message}\n\n` +
                     `Vui lÃ²ng kiá»ƒm tra láº¡i thÃ´ng tin vÃ  thá»­ láº¡i.`,
            suggestions: ['Thá»­ láº¡i', 'VÃ­ dá»¥ tÃ­nh toÃ¡n'],
            productRecommendations: [],
            confidence: 0.5
          }
        }
      }
      // ===== RECOMMENDATION FLOW =====
      else if ((lowerMessage.includes('gá»£i Ã½') || lowerMessage.includes('Ä‘á» xuáº¥t') || 
           lowerMessage.includes('recommend')) && customerId) {
        console.log('ðŸ’¡ Generating personalized recommendations...')
        
        try {
          // Get ML personalized recommendations
          const mlScores = await mlRecommendations.getHybridRecommendations(
            undefined, // No specific product
            customerId,
            'PERSONALIZED',
            5
          )
          
          const recommendations = await mlRecommendations.enrichRecommendations(mlScores)
          
          botResponse = {
            response: `ðŸ’¡ **Dá»±a trÃªn lá»‹ch sá»­ mua hÃ ng cá»§a báº¡n**, tÃ´i gá»£i Ã½ cÃ¡c sáº£n pháº©m nÃ y:\n\n` +
                     `CÃ¡c sáº£n pháº©m bÃªn dÆ°á»›i phÃ¹ há»£p vá»›i nhu cáº§u vÃ  dá»± Ã¡n cá»§a báº¡n. ` +
                     `Báº¡n cÃ³ thá»ƒ xem chi tiáº¿t hoáº·c há»i tÃ´i thÃªm vá» báº¥t ká»³ sáº£n pháº©m nÃ o!`,
            suggestions: [
              'Chi tiáº¿t sáº£n pháº©m Ä‘áº§u tiÃªn',
              'So sÃ¡nh giÃ¡',
              'TÃ­nh toÃ¡n váº­t liá»‡u',
              'Xem thÃªm gá»£i Ã½'
            ],
            productRecommendations: recommendations,
            confidence: 0.9
          }
        } catch (mlError) {
          console.error('ML recommendations failed:', mlError)
          // Fallback to regular chatbot response
          botResponse = await generateChatbotResponse(message, context, formattedHistory)
        }
      }
      // ===== REGULAR CHAT FLOW =====
      else {
        botResponse = await generateChatbotResponse(message, context, formattedHistory)
      }
    }

    // Log customer interaction
    await prisma.customerInteraction.create({
      data: {
        customerId,
        sessionId,
        interactionType: 'CHATBOT',
        productId: context?.productId,
        query: actualQuery,
        response: botResponse.response,
        metadata: {
          confidence: botResponse.confidence,
          suggestions: botResponse.suggestions,
          productRecommendations: botResponse.productRecommendations,
          hasImage: !!image,
          recognitionData: recognitionResult,
          context
        },
        ipAddress: request.headers.get('x-forwarded-for') || 
                   request.headers.get('x-real-ip') || 
                   'unknown',
        userAgent: request.headers.get('user-agent') || 'unknown'
      }
    })

    const response = {
      message: botResponse.response,
      suggestions: botResponse.suggestions,
      productRecommendations: botResponse.productRecommendations,
      confidence: botResponse.confidence,
      recognitionData: botResponse.recognitionData,
      sessionId,
      timestamp: new Date().toISOString()
    }

    return NextResponse.json(
      createSuccessResponse(response, 'Chatbot response generated successfully'),
      { status: 200 }
    )

  } catch (error) {
    console.error('Chatbot error:', error)
    return NextResponse.json(
      createErrorResponse('Internal server error', 'INTERNAL_ERROR'),
      { status: 500 }
    )
  }
}

// GET /api/chatbot - Get chat history
export async function GET(request: NextRequest) {
  try {
    const { searchParams } = new URL(request.url)
    const sessionId = searchParams.get('sessionId')
    const customerId = searchParams.get('customerId')
    const limit = parseInt(searchParams.get('limit') || '50')

    if (!sessionId) {
      return NextResponse.json(
        createErrorResponse('Session ID is required', 'VALIDATION_ERROR'),
        { status: 400 }
      )
    }

    // Build where clause
    const where: any = {
      sessionId,
      interactionType: 'CHATBOT'
    }

    if (customerId) {
      where.customerId = customerId
    }

    // Get chat history
    const interactions = await prisma.customerInteraction.findMany({
      where,
      orderBy: { createdAt: 'asc' },
      take: limit,
      select: {
        id: true,
        query: true,
        response: true,
        metadata: true,
        createdAt: true
      }
    })

    // Format chat history
    const chatHistory = interactions.map(interaction => ({
      id: interaction.id,
      userMessage: interaction.query,
      botMessage: interaction.response,
      suggestions: (interaction.metadata as any)?.suggestions || [],
      productRecommendations: (interaction.metadata as any)?.productRecommendations || [],
      confidence: (interaction.metadata as any)?.confidence || 0,
      timestamp: interaction.createdAt
    }))

    return NextResponse.json(
      createSuccessResponse(chatHistory, 'Chat history retrieved successfully'),
      { status: 200 }
    )

  } catch (error) {
    console.error('Get chat history error:', error)
    return NextResponse.json(
      createErrorResponse('Internal server error', 'INTERNAL_ERROR'),
      { status: 500 }
    )
  }
}
